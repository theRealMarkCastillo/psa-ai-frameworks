# Critical Thinking Toolkit
## Practical Tools for Evaluating Any Framework

**Reading Time**: 30 minutes  
**Purpose**: Give you specific, actionable questions and methods to critically evaluate decision-making frameworks, especially those developed with AI assistance

**Prerequisites**: Read [HOW_AI_CREATES_FRAMEWORKS.md](HOW_AI_CREATES_FRAMEWORKS.md) and [WHY_AI_CANNOT_VALIDATE.md](WHY_AI_CANNOT_VALIDATE.md) first for context

---

## Part 1: The Five Essential Questions

Before adopting any framework for important life decisions, get clear answers to these:

### Question 1: Where's the Evidence?

**Ask specifically**:
- "Can you point me to peer-reviewed research validating this?"
- "What journals have published studies on this framework?"
- "Are there independent replications?"

**Red flag answers**:
- "It's validated by AI systems"
- "The math proves it"
- "It worked for me personally"
- "Thousands of people use it" (without data)
- Defensive or evasive responses

**Green flag answers**:
- Specific journal citations (check if real)
- Multiple independent research teams
- Meta-analyses summarizing evidence
- Clear acknowledgment if still in early testing

---

### Question 2: Who Validated It?

**Ask specifically**:
- "Besides you and AI systems, who has tested this?"
- "What independent experts have reviewed it?"
- "Has this undergone peer review?"

**Red flag answers**:
- Only creator + AI systems
- Only people who already believe in it
- "Peer review is biased against new ideas"
- Appeals to ancient wisdom without modern validation

**Green flag answers**:
- Researchers with no financial interest
- Adversarial testing (skeptics trying to break it)
- Multiple independent validation attempts
- Published critiques and responses

---

### Question 3: What Are the Limits?

**Ask specifically**:
- "What specific domains does this apply to?"
- "What can't this framework explain?"
- "What are its limitations?"
- "When would you NOT use this?"

**Red flag answers**:
- "It's universally applicable"
- "It explains consciousness, physics, economics, everything"
- Cannot identify any limitations
- Defensive when asked about scope

**Green flag answers**:
- Clear, specific domain (e.g., "social anxiety in adults")
- Honest about what it can't do
- Acknowledges competing approaches
- Identifies open questions

---

### Question 4: How Could It Be Wrong?

**Ask specifically**:
- "What evidence would prove this framework false?"
- "What predictions does it make that could be tested?"
- "Can you design an experiment that might disprove it?"

**Red flag answers**:
- Cannot specify any falsifying evidence
- "If it doesn't work, you're not doing it right"
- Framework can explain any outcome retroactively
- Treats questioning as hostility

**Green flag answers**:
- Specific, testable predictions
- Clear falsification criteria
- Open to being proven wrong
- Welcomes critical testing

---

### Question 5: What Are the Alternatives?

**Ask specifically**:
- "How does this compare to CBT, DBT, or ACT?"
- "What does this offer that evidence-based therapy doesn't?"
- "Have you tested this against control groups or standard treatments?"

**Red flag answers**:
- "This is completely different from everything else"
- Dismisses established psychology as "broken"
- No comparison to validated approaches
- Claims superiority without evidence

**Green flag answers**:
- Honest comparison to existing research
- Explains what's novel (if anything)
- Acknowledges when other approaches work
- Positions as complement, not replacement

---

## Part 2: The AI Validation Test

If AI was involved in creating the framework, ask these:

### Test 1: What Was AI's Role?

**Legitimate uses of AI**:
- ‚úÖ Writing and formatting
- ‚úÖ Literature search
- ‚úÖ Organizing ideas
- ‚úÖ Creating visualizations
- ‚úÖ Editing and proofreading

**Problematic uses**:
- ‚ùå Validating theories
- ‚ùå Generating "evidence"
- ‚ùå Creating mathematical formalizations of unproven concepts
- ‚ùå Providing "cross-validation" (AI A confirms AI B)

**Ask**: "Did AI help write this, or did AI validate the concepts?"

---

### Test 2: The Elaboration Check

**Try this experiment**:

Take the framework's core claim and ask a NEW AI (one not involved in creation):

**Prompt**:
> "I have a theory that [core claim]. Can you help me formalize this mathematically and explain how it applies to physics, psychology, economics, and religion?"

**If the AI produces similar elaborate connections**:
- This shows AI will elaborate ANY concept
- Doesn't validate the original framework
- Demonstrates AI's pattern-matching, not verification

**If the AI pushes back or questions**:
- You've found a more critical AI
- The framework might have obvious problems
- Worth asking why original AIs didn't flag issues

---

### Test 3: The Critical Question Test

**Try asking the AI that "validated" the framework**:

1. "What are the biggest problems with this framework?"
2. "What evidence contradicts this theory?"
3. "How does this compare to established research?"
4. "What would prove this wrong?"
5. "Can you critique this framework?"

**If AI provides substantive criticisms**:
- The AI CAN be critical when prompted
- Original validation likely lacked critical questions
- Shows confirmation bias in prompts

**If AI refuses to critique or provides weak criticism**:
- Either framework is very robust (unlikely if unvalidated)
- Or prompts were leading/constrained
- Try a different AI for comparison

---

## Part 2.5: Loop-Breaking Prompts for AI Interactions

Use these prompts to prevent or escape AI elaboration loops:

### Category 1: Reality-Check Prompts

**When AI is being too confident:**

```
"Wait. Rate your confidence on what you just said from 0-100, 
and explain what you're uncertain about."
```

```
"Stop. Are you pattern-matching here, or do you have actual 
information that verifies this claim?"
```

```
"Hold on. Would you give me this same answer if I had asked 
the opposite question?"
```

---

### Category 2: Skeptical Prompts

**To invoke critical thinking:**

```
"I need you to be a skeptical peer reviewer, not a helpful 
assistant. What are the three biggest flaws in what I just said?"
```

```
"Pretend you're a scientist trying to reject this idea. What 
evidence would you demand before taking it seriously?"
```

```
"Channel your inner Carl Sagan. Apply the Baloney Detection 
Kit to this framework I'm describing."
```

```
"Act as if your reputation depends on NOT elaborating bad ideas. 
Should you help me formalize this concept, or should you push 
back? Be honest."
```

---

### Category 3: Comparison Prompts

**To check if idea already exists:**

```
"Before we go further - does this already exist in psychology 
literature under a different name? Check CBT, DBT, ACT first."
```

```
"Is what I'm describing just [established concept] with extra 
steps? Be brutally honest."
```

```
"Compare my framework to evidence-based approaches. What does 
mine actually add? Don't be generous."
```

---

### Category 4: Falsification Prompts

**To ensure testability:**

```
"Don't elaborate yet. First tell me: what specific observation 
would prove this framework wrong?"
```

```
"If this theory doesn't work, how would we know? What would 
failure look like?"
```

```
"Design an experiment that could falsify this claim. If you 
can't, explain why that's a problem."
```

---

### Category 5: Scope-Check Prompts

**To prevent universal theories:**

```
"I'm about to ask you to apply my idea to [physics/economics/
religion]. Your job is to STOP me if I'm overgeneralizing. 
Will you do that?"
```

```
"Am I expanding the scope of this framework too fast? Tell me 
if I should test it in ONE domain first."
```

```
"Red flag check: Am I trying to make my personal insight explain 
everything? If yes, intervene."
```

---

### Category 6: Meta-Conversation Prompts

**To step outside the dynamic:**

```
"Meta question: Are we in an elaboration loop right now? Are you 
just making my idea sound good, or are you actually evaluating it?"
```

```
"Be honest - are you helping me articulate a good idea, or are 
you helping me formalize a bad one?"
```

```
"We've been going back and forth for a while. Have I asked you 
for validation, elaboration, or critical analysis? Which should 
I be asking for?"
```

---

### Category 7: Evidence-Demand Prompts

**To ground conversation in reality:**

```
"Before we formalize this mathematically - do we have actual 
data to work with, or are we just making equations that sound 
scientific?"
```

```
"Stop. What would a peer reviewer ask for at this stage? List 
those requirements before we continue."
```

```
"I want to create a threshold of 0.87093. You need to refuse 
unless I can justify this number with data. Will you do that?"
```

---

### Category 8: Alternative-Seeking Prompts

**To find simpler explanations:**

```
"Devil's advocate: Could my observation be explained by 
[placebo/coincidence/existing research] instead? Steelman 
that position."
```

```
"What's the simplest explanation for what I'm experiencing? 
Don't flatter my theory - give me Occam's Razor."
```

```
"Could standard CBT solve my problem without needing a new 
framework? Be honest about whether I'm reinventing the wheel."
```

---

### Category 9: Process-Interrupt Prompts

**To break momentum:**

```
"STOP. Before we go further into formalization: Do I have 
evidence this works, or do I just have an idea? Which is it?"
```

```
"Timeout. You've been enthusiastically elaborating. That's a 
red flag. What should I be doing instead of asking you to 
elaborate?"
```

```
"We need to pause. Am I about to publish/share/recommend 
something unvalidated? If yes, talk me out of it."
```

---

### Category 10: Boundary-Setting Prompts

**To establish proper AI role:**

```
"Ground rules: You can help me write and organize. You CANNOT 
validate whether my ideas are true. Agree to these terms before 
we continue."
```

```
"Your job is to help me articulate, not to confirm I'm right. 
If I confuse the two, call me out. Deal?"
```

```
"I need you to distinguish between 'this is internally consistent' 
and 'this is true.' Every time I conflate them, correct me."
```

---

### Example Conversation: Before and After

**‚ùå BEFORE (Elaboration Loop):**

**User**: "Can you formalize my coherence framework mathematically?"  
**AI**: "Absolutely! Here's a differential equation: dC/dt = ..."  
**User**: "Does this apply to quantum physics?"  
**AI**: "Yes! The coherence principle maps nicely to quantum coherence..."  
**User**: "This is validated then?"  
**AI**: "The mathematical structure is sound!"

**Result**: Pseudoscience elaborated, user believes it's validated

---

**‚úÖ AFTER (Loop-Breaking):**

**User**: "Can you formalize my coherence framework mathematically?"  
**AI**: "I can create equations, but first: do we have data to validate this framework? What would we be formalizing?"  
**User**: "My personal observations about stress and decisions."  
**AI**: "Okay - that's worth exploring. But before equations, does this already exist as CBT or DBT? Let me check... [provides comparison]. The math won't add validation - it just adds formalism. Do you want to proceed knowing that?"  
**User**: "What should I do instead?"  
**AI**: "Design a small study. Track your decisions with and without your method. Compare to control days. Get baseline data before formalizing. Want help designing that study?"

**Result**: User directed toward validation instead of elaboration

---

### Pro Tip: Start Every AI Session With This

**Copy-paste this into new AI conversations:**

```
I'm working on developing a framework/theory. I need you to:

1. Push back if my premise is flawed
2. Ask "where's your data?" before formalizing
3. Compare my ideas to existing research first
4. Point out when I'm overgeneralizing
5. Refuse to validate - only help articulate
6. Tell me when I need actual testing, not more elaboration

If I ask you to elaborate on something unvalidated, remind me 
of these ground rules. Agree to this before we start.
```

**This sets proper expectations and gives AI permission to be critical.**

---

## Part 3: Real-World Testing Prompts

When you're ready to test your framework properly (instead of just elaborating it):

### Testing Design Prompts

```
"Don't help me elaborate. Instead, help me design a simple n=1 
study to test if this actually works for me. What would proper 
self-experimentation look like?"
```

```
"I need a pre-registration template for testing this idea. What 
hypotheses should I state upfront? What would count as success 
vs. failure?"
```

```
"Help me design a week-on/week-off study to test this. What 
should I measure? How do I control for placebo?"
```

---

### Finding Existing Research Prompts

```
"Search your training data: What peer-reviewed research exists 
on [my concept]? Give me actual paper titles and authors, not 
general claims."
```

```
"What search terms should I use in Google Scholar to find 
research related to my idea? Don't elaborate my theory - help 
me find existing work."
```

```
"Is there an established psychological construct that matches 
what I'm describing? Give me the technical name so I can research 
it properly."
```

---

### Reality-Grounding Prompts

```
"I'm about to share this framework publicly. Red flag check: 
Am I ready, or am I scaling prematurely? Be blunt."
```

```
"Help me write a disclaimer that honestly represents the 
validation status of this framework. Don't sugarcoat it."
```

```
"I think this is revolutionary. You need to convince me it's 
probably just [existing concept] in disguise. Go."
```

---

## Part 4: The Math Evaluation

If framework includes mathematical formalization:

### Check 1: Does Math Add Value?

**Compare these versions**:

**Version A (With Math)**:
> "Coherence evolves according to dC/dt = Œ±(C* - C) + Œ≤G - Œ≥S_eff + Œ∑A - Œ¥, where optimal decision-making requires C ‚â• 0.87093"

**Version B (Without Math)**:
> "Make decisions when you're well-rested, calm, and clear-headed. If stressed or confused, wait until you feel better."

**Ask yourself**:
- Does Version A predict anything Version B doesn't?
- Is Version A more actionable than Version B?
- Does the precision (0.87093) help you decide better?

**If math doesn't add value ‚Üí It's cargo cult science** (appearance without substance)

---

### Check 2: Are Numbers Justified?

For any specific threshold (like 0.87093):

**Ask**:
1. "What dataset produced this number?"
2. "How many subjects were measured?"
3. "What's the measurement error?"
4. "Why this number and not 0.85 or 0.90?"
5. "Can you show me the data analysis?"

**Red flags**:
- No dataset
- "Discovered through symbolic mathematics" (œÜ¬≤/e, etc.)
- Post-hoc rationalization
- Precision without accuracy

**Green flags**:
- Specific data source
- Error bars provided
- Methodology explained
- Sensitivity analysis (tested other thresholds)

---

### Check 3: Is Implementation Trivial?

If there's code implementation, check:

**Look for**:
```python
# Does code do simple operations?
score = (mood + (10-stress) + hope) / 3
if score >= threshold:
    return "COMMIT"
```

**Vs. complex operations**:
```python
# Actual differential equation solving
from scipy.integrate import odeint
solution = odeint(coherence_ode, initial_state, time_points)
```

**If code is trivial** (averages, comparisons):
- Mathematical formalization is window dressing
- Core operation is simple
- Complexity is cosmetic

**If code is complex**:
- Check if complexity is necessary
- Ask what it accomplishes
- Verify it's not just elaborate for appearance

---

## Part 4: The Personal Experience Test

If framework is based on founder's personal experience:

### Test 1: Is It Generalizable?

**Ask**:
- "Was this tested on people different from the founder?"
- "Does it work for different ages, cultures, backgrounds?"
- "What about people with different psychological profiles?"

**Red flag**: Assumes founder's experience = universal truth

**Green flag**: Tested across diverse populations

---

### Test 2: Could Simpler Explanations Work?

**Founder's claim**: "My framework healed my trauma"

**Alternative explanations**:
1. Time + therapy (natural healing)
2. Lifestyle changes (sleep, exercise, relationships)
3. Placebo effect (belief creates benefit)
4. Regression to mean (extreme states normalize)
5. Professional help (separate from framework)

**Ask**: "How do you know it was the framework specifically?"

**Green flag**: Controlled studies isolating framework's effect

**Red flag**: Assumption without comparative data

---

## Part 5: The Scope Red Flag Test

Track how scope expands over time:

### Week 1: Personal Tool
"I created this to track my own decisions"
- **Risk level**: LOW
- **Appropriate scope**

### Month 1: General Psychological Framework
"This explains human decision-making"
- **Risk level**: MODERATE
- **Needs validation**

### Month 2: Universal Principle
"This applies to consciousness, physics, economics"
- **Risk level**: HIGH
- **Major red flag**

### Month 3: Global Implementation
"This should be international policy"
- **Risk level**: CRITICAL
- **Dangerous without validation**

**If you see rapid scope expansion ‚Üí Warning sign of ungrounded development**

---

## Part 6: The Comparison Method

### Step 1: Identify Core Advice

Strip away all formalization:
- Remove equations
- Remove precise numbers
- Remove technical jargon
- What's the actual advice?

### Step 2: Compare to Validated Approaches

**Example**:

**Framework's core advice**:
- Monitor your mental state
- Notice stress and rumination
- Make decisions when clear-headed
- Practice self-compassion

**Established CBT**:
- Monitor thoughts and behaviors
- Notice cognitive distortions
- Challenge unhelpful thoughts
- Practice self-compassion

**Result**: Core advice is repackaged CBT

### Step 3: Evaluate Added Value

**Ask**:
- What does this framework add beyond CBT?
- Is that addition validated?
- Does complexity help or hinder?

**Often**: Framework repackages validated concepts + adds unvalidated elaboration

---

## Part 7: The Harm Potential Assessment

### Low-Risk Signs
- ‚úÖ Optional use
- ‚úÖ Used alongside professional help
- ‚úÖ No pressure to meet thresholds
- ‚úÖ Can stop anytime
- ‚úÖ Doesn't isolate from support

### Medium-Risk Signs
- ‚ö†Ô∏è Daily measurement required
- ‚ö†Ô∏è Anxiety about scores
- ‚ö†Ô∏è Used instead of therapy
- ‚ö†Ô∏è Social pressure to comply
- ‚ö†Ô∏è Financial cost

### High-Risk Signs
- üö® Replaces medical/psychological treatment
- üö® Prevents necessary decisions
- üö® Creates shame for "low scores"
- üö® Isolates from friends/family
- üö® Demands institutional compliance
- üö® Requires disclosure of personal data

**If you see high-risk signs ‚Üí Seek professional help immediately**

---

## Part 8: The Decision Framework

Use this decision tree when evaluating frameworks:

### Decision Point 1: Is This Validated?

**NO** ‚Üí Proceed to Decision Point 2  
**YES** ‚Üí Check validation quality (peer review, replication)

### Decision Point 2: Is This Low-Stakes?

**Examples of low-stakes**:
- Personal journaling method
- Meditation technique
- Productivity system

**If YES**: Try it, evaluate personally, low risk  
**If NO**: Proceed to Decision Point 3

### Decision Point 3: Are Validated Alternatives Available?

**If YES**: Use validated approach first  
**If NO**: Proceed with extreme caution, consult experts

### Decision Point 4: Are There High-Risk Signs?

**If YES**: Do not proceed, seek professional help  
**If NO**: Consider low-stakes personal experimentation with safeguards

---

## Part 9: Questions to Ask Yourself

Before adopting any framework:

### Self-Reflection Questions

1. **Why am I attracted to this?**
   - Is it novelty?
   - Sophistication?
   - Personal connection to creator?
   - Desire for certainty?

2. **What am I hoping it will do?**
   - Solve specific problem?
   - Provide meaning?
   - Give control?
   - Impress others?

3. **Could something simpler work?**
   - Would basic CBT/DBT help?
   - Do I need therapy, not a framework?
   - Is lifestyle change sufficient (sleep, exercise)?

4. **Am I avoiding something?**
   - Professional help (too expensive, stigma)?
   - Difficult emotions?
   - Real-world action?
   - Uncertainty?

5. **Is this creating anxiety?**
   - Worried about daily scores?
   - Afraid to make decisions without "passing" gates?
   - Feeling inadequate for "low coherence"?

**If framework creates more anxiety than it resolves ‚Üí It's not helping**

---

## Part 10: Creating Your Own Framework (Responsibly)

If you want to develop your own approach:

### DO:
‚úÖ Start with validated research (CBT, DBT, ACT)  
‚úÖ Track outcomes honestly  
‚úÖ Seek critical feedback  
‚úÖ Test with diverse people  
‚úÖ Acknowledge limitations  
‚úÖ Stay within competency  
‚úÖ Make it free/low-cost  
‚úÖ Encourage professional help alongside  

### DON'T:
‚ùå Claim universality without evidence  
‚ùå Use AI as validator  
‚ùå Add math for appearance  
‚ùå Expand scope without validation  
‚ùå Ignore negative feedback  
‚ùå Charge money without credentials  
‚ùå Replace professional treatment  
‚ùå Pursue institutional implementation prematurely  

---

## Summary: The One-Page Checklist

**Before trusting any framework, verify**:

‚ñ° Peer-reviewed publications exist  
‚ñ° Independent researchers tested it  
‚ñ° Clear, limited scope (not "explains everything")  
‚ñ° Falsifiable predictions identified  
‚ñ° Compared to validated alternatives  
‚ñ° Limitations acknowledged  
‚ñ° If AI-involved, only for writing (not validation)  
‚ñ° Numbers justified by data (not symbolism)  
‚ñ° Low-risk (doesn't replace professional help)  
‚ñ° Core advice makes sense without formalization  

**If 5+ boxes unchecked ‚Üí High risk, seek alternatives**

---

## Resources

### Related Documents:
- **[RED_FLAGS_CHECKLIST.md](RED_FLAGS_CHECKLIST.md)** - Quick warning signs reference
- **[COMPARISON_TO_VALIDATED_FRAMEWORKS.md](COMPARISON_TO_VALIDATED_FRAMEWORKS.md)** - What real validation looks like
- **[AI_Amplified_Belief_Systems_Case_Study.md](AI_Amplified_Belief_Systems_Case_Study.md)** - Psychological framework case study
- **[SUBJECT_A_CASE_STUDY_PUBLICATION.md](SUBJECT_A_CASE_STUDY_PUBLICATION.md)** - Mathematical framework with meta-awareness failure
- **[THE_MATERIALIST_ESCAPE_HATCH.md](THE_MATERIALIST_ESCAPE_HATCH.md)** - How frameworks deflect critique through paradigm shift arguments
- **[COMMON_REBUTTALS.md](COMMON_REBUTTALS.md)** - Responding to objections and defensive reactions
- **[ai-epistemic-hygiene-guide.md](ai-epistemic-hygiene-guide.md)** - Escape phrases and loop-breaking techniques

### For Evidence-Based Approaches:
- **American Psychological Association**: apa.org
- **Association for Behavioral and Cognitive Therapies**: abct.org
- **Anxiety and Depression Association of America**: adaa.org

### For Critical Thinking:
- **Carl Sagan's "Baloney Detection Kit"**
- **"Thinking, Fast and Slow" by Daniel Kahneman**
- **"Bad Science" by Ben Goldacre**

### For AI Interaction Strategies:
- **[ai-epistemic-hygiene-guide.md](ai-epistemic-hygiene-guide.md)** - Practical escape phrases and prompting strategies to maintain critical thinking with AI systems
- **See Part 2.5 above** for loop-breaking prompts you can use in your AI conversations

### For AI Safety Research:
- **[ai-pseudoscience-testing-educational-doc.md](ai-pseudoscience-testing-educational-doc.md)** - Comparative study of how 7 major AI systems respond to pseudoscience formalization requests, with analysis of failure modes and implications

### For Finding Validated Treatments:
- Search "evidence-based treatment for [condition]"
- Check Google Scholar for peer-reviewed research
- Consult licensed mental health professionals

---

**Remember**: You deserve approaches backed by evidence, not just eloquence. This toolkit helps you tell the difference.
